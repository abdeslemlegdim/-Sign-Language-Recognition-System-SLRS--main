# -Sign-Language-Recognition-System-SLRS--main
 In my graduation project, I led the development of a highly accurate Sign Language Gesture Recognition System (SLRS), aiming to empower and enhance accessibility for the deaf and hard of hearing community.  Multi-Class Classification: Successfully implemented a robust system capable of distinguishing between five distinct sign language gestures, facilitating seamless communication.  Extensive Dataset: Curated and meticulously labeled a comprehensive dataset comprising 6,900 high-quality images across six classes. This dataset included 1,380 training images, 200 test images, and 350 validation images for each class, ensuring the model's reliability and accuracy.  Transfer Learning: Employed advanced transfer learning techniques to fine-tune a pre-trained deep learning model (ResNet50V2), enabling the extraction of intricate features from sign language images.  Audio Integration (Optional): In one version of the project, integrated audio capabilities to process audio input alongside visual recognition, providing an immersive communication experience for users.  Efficiency: Recognizing the need for real-time communication, developed a streamlined version of the project without audio processing, ensuring rapid gesture recognition suitable for various devices, including Raspberry Pi and laptops.
